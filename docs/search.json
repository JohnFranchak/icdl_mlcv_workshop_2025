[
  {
    "objectID": "program.html",
    "href": "program.html",
    "title": "Detailed Program",
    "section": "",
    "text": "The first talk session will discuss using machine learning to track infants’ behavior. Most research about infants’ everyday behavior has relied on laborious human coding of video recordings. Recent advances in wearable technologies—movement sensors, ECG, headcams/eye trackers, and audio recorders—have enormous potential to measure continuous, full-day behavior. However, sensors do not directly measure behaviors of psychological interest, such as sleep/wake states, motor actions, joint attention, and infant-caregiver proximity. Speakers will demonstrate how they have leveraged machine learning algorithms to classify infant and caregiver behavior from wearable sensors.\n\n\n\n\n\n\nKathryn Humphreys (Vanderbilt University, US)\n\n\n\n\n\nMy talk will discuss capturing the dynamics of caregiver–child proximity across early life using wearable sensing technology. Our lab developed the TotTag, a wearable device that measures physical proximity between caregivers and children twice per second, allowing unprecedented ecological assessment of how proximity patterns differ across families and contexts, change across development, and relate to developmental outcomes. This approach enables continuous monitoring of caregiver–child interactions in natural settings. We are currently using this methodology in longitudinal studies tracking family proximity patterns from pregnancy through early childhood, capturing everyday experiences during developmental transitions that were previously difficult to measure outside laboratory settings.\n\n\n\n\n\n\n\n\n\nJohn Franchak, Hanzhi Wang, & Hailey Rousey (University of California, Riverside, US)\n\n\n\n\n\nWearable movement sensors have the potential to describe real-time infant motor behavior in naturalistic settings, but raw sensor data need to be classified into relevant categories. We will present novel machine learning classifiers for infant posture (e.g., sitting, standing), locomotion (e.g., crawling, walking), and restraint (e.g., placed in seat and/or held) and describe results about each behavior from a dataset of 850 hours of in-home recording.\n\n\n\n\n\n\n\n\n\nNancy McElwain (University of Illinois, Urbana-Champaign, US)\n\n\n\n\n\nOur interdisciplinary team has developed LittleBeats, an infant wearable platform that synchronously collects multimodal data (e.g., ECG, audio, motion) for extended periods of time (e.g., daylong recordings) in real-world contexts (e.g., the home). We use multimodal deep learning algorithms applied to LittleBeats data to assess a variety constructs, including infants’ regulation of emotions, sleep/wake states, and physical activity/sedentary behavior. We will present an example of these multimodal deep learning architectures for the detection of infant wake/sleep states, including REM and non-REM sleep stages. Advantages and challenges of multimodal machine learning approaches will be discussed.\n\n\n\n\n\n\n\n\n\nMarwa Mahmoud (University of Glasgow, UK)\n\n\n\n\n\nAutomatic detection of behavioral cues from video offers promising tools for supporting the early diagnosis of neurodevelopmental conditions. In this talk, I will present our work on detecting specific infant movements—such as face-touch gestures—from video recordings, enabling insights into how babies engage with themselves and their environment. I will also discuss our results on the automatic analysis of videos during the Strange Situation Procedure to predict child attachment scores. In addition, I will present a multimodal machine learning framework that processes multiple behavioral signals to model behavior more holistically. Finally, I will highlight our recent efforts on interpretable, concept-based deep learning approaches, emphasising the importance of explainability in child behavior analysis.\n\n\n\n\n\n\n\n\n\nManu Airaksinen (Helsinki, Finland)\n\n\n\n\n\nEveryday movement behavior is exhibited at vastly varying temporal scales ranging from micro-scale (~seconds) to macro-scale (~minutes to ~hours), where the higher-scale units (e.g., “playing football”) causally condition the observed statistics of the lower-scale units (e.g., “instantaneous posture”). By focusing only on any single level of analysis, crucially important contextual information is lost. Thus, a grand challenge in improving real-world behavioral analytics is to find the relevant and robust activity recognition targets for all temporal scales. Here, a particular challenge for inertial sensors is to reconcile the mismatch between the measured signals and the visually interpreted content of the event."
  },
  {
    "objectID": "program.html#talk-session-1-using-machine-learning-to-track-behavior",
    "href": "program.html#talk-session-1-using-machine-learning-to-track-behavior",
    "title": "Detailed Program",
    "section": "",
    "text": "The first talk session will discuss using machine learning to track infants’ behavior. Most research about infants’ everyday behavior has relied on laborious human coding of video recordings. Recent advances in wearable technologies—movement sensors, ECG, headcams/eye trackers, and audio recorders—have enormous potential to measure continuous, full-day behavior. However, sensors do not directly measure behaviors of psychological interest, such as sleep/wake states, motor actions, joint attention, and infant-caregiver proximity. Speakers will demonstrate how they have leveraged machine learning algorithms to classify infant and caregiver behavior from wearable sensors.\n\n\n\n\n\n\nKathryn Humphreys (Vanderbilt University, US)\n\n\n\n\n\nMy talk will discuss capturing the dynamics of caregiver–child proximity across early life using wearable sensing technology. Our lab developed the TotTag, a wearable device that measures physical proximity between caregivers and children twice per second, allowing unprecedented ecological assessment of how proximity patterns differ across families and contexts, change across development, and relate to developmental outcomes. This approach enables continuous monitoring of caregiver–child interactions in natural settings. We are currently using this methodology in longitudinal studies tracking family proximity patterns from pregnancy through early childhood, capturing everyday experiences during developmental transitions that were previously difficult to measure outside laboratory settings.\n\n\n\n\n\n\n\n\n\nJohn Franchak, Hanzhi Wang, & Hailey Rousey (University of California, Riverside, US)\n\n\n\n\n\nWearable movement sensors have the potential to describe real-time infant motor behavior in naturalistic settings, but raw sensor data need to be classified into relevant categories. We will present novel machine learning classifiers for infant posture (e.g., sitting, standing), locomotion (e.g., crawling, walking), and restraint (e.g., placed in seat and/or held) and describe results about each behavior from a dataset of 850 hours of in-home recording.\n\n\n\n\n\n\n\n\n\nNancy McElwain (University of Illinois, Urbana-Champaign, US)\n\n\n\n\n\nOur interdisciplinary team has developed LittleBeats, an infant wearable platform that synchronously collects multimodal data (e.g., ECG, audio, motion) for extended periods of time (e.g., daylong recordings) in real-world contexts (e.g., the home). We use multimodal deep learning algorithms applied to LittleBeats data to assess a variety constructs, including infants’ regulation of emotions, sleep/wake states, and physical activity/sedentary behavior. We will present an example of these multimodal deep learning architectures for the detection of infant wake/sleep states, including REM and non-REM sleep stages. Advantages and challenges of multimodal machine learning approaches will be discussed.\n\n\n\n\n\n\n\n\n\nMarwa Mahmoud (University of Glasgow, UK)\n\n\n\n\n\nAutomatic detection of behavioral cues from video offers promising tools for supporting the early diagnosis of neurodevelopmental conditions. In this talk, I will present our work on detecting specific infant movements—such as face-touch gestures—from video recordings, enabling insights into how babies engage with themselves and their environment. I will also discuss our results on the automatic analysis of videos during the Strange Situation Procedure to predict child attachment scores. In addition, I will present a multimodal machine learning framework that processes multiple behavioral signals to model behavior more holistically. Finally, I will highlight our recent efforts on interpretable, concept-based deep learning approaches, emphasising the importance of explainability in child behavior analysis.\n\n\n\n\n\n\n\n\n\nManu Airaksinen (Helsinki, Finland)\n\n\n\n\n\nEveryday movement behavior is exhibited at vastly varying temporal scales ranging from micro-scale (~seconds) to macro-scale (~minutes to ~hours), where the higher-scale units (e.g., “playing football”) causally condition the observed statistics of the lower-scale units (e.g., “instantaneous posture”). By focusing only on any single level of analysis, crucially important contextual information is lost. Thus, a grand challenge in improving real-world behavioral analytics is to find the relevant and robust activity recognition targets for all temporal scales. Here, a particular challenge for inertial sensors is to reconcile the mismatch between the measured signals and the visually interpreted content of the event."
  },
  {
    "objectID": "program.html#talk-session-2-using-machine-learning-to-track-environmentsaudio-and-video",
    "href": "program.html#talk-session-2-using-machine-learning-to-track-environmentsaudio-and-video",
    "title": "Detailed Program",
    "section": "Talk Session 2: Using machine learning to track environments—Audio and video",
    "text": "Talk Session 2: Using machine learning to track environments—Audio and video\nThe second talk session will discuss using machine learning to track early environments. Infants interact with their caregivers in context, and this environmental context crucially shapes developmental outcomes. For example, caregivers’ speech input and sensitivity, as well as broader features of environments (e.g., noise, household chaos) can impact language and socio-emotional learning. How can researchers capture these environmental features using audio and video recordings? Speakers will discuss how they have applied machine learning algorithms to capture the early auditory and visual environment.\n\n\n\n\n\n\nManjunath Mulimani (Tampere University, Finland)\n\n\n\n\n\nThis talk will present the ongoing research works on detection and classification of acoustic scene and events (DCASE) at the Signal Processing Research Centre. I will introduce acoustic scene classification, audio tagging and sound event detection. Further, I will highlight the issues and solutions associated with deep learning models for learning the DCASE tasks incrementally over existing knowledge. I will discuss the available audio pre-trained models, ways to extract the embeddings and solve target DCASE tasks. Finally, I will list all ongoing research works in our Audio Research Group and discuss collaboration opportunities.\n\n\n\n\n\n\n\n\n\nKaijia Tey (Paris, France)\n\n\n\n\n\nLong-form recordings let us capture the richness of children’s everyday language environments, offering insight into how they hear and use language in real life. Using tools like LENA and open-source alternatives (e.g., Voice Type Classifier (VTC), Lavechin et al. 2020), we measure key aspects of early language experience, including child vocalizations, adult speech, and conversational turns. I will share how we collect and analyze these recordings, what they reveal about language development across diverse communities, and the challenges we still face. This work highlights the potential and limitations of current methods in capturing children’s linguistic experiences.\n\n\n\n\n\n\n\n\n\nEinari Vaaras (Tampere University, Finland)\n\n\n\n\n\nUnderstanding the emotional content of speech heard by infants is crucial for studying early emotional experiences and their impact on infants’ developmental outcomes. This talk will present a state-of-the-art approach for automatic emotional analysis of child-directed speech in a neonatal intensive care unit environment. I will cover the development process of the approach, including data collection, the main challenges encountered, and the solutions implemented to address them. Additionally, I will briefly discuss the performance level of the proposed approach and the applications where the approach has been used.\n\n\n\n\n\n\n\n\n\nPierre Labendzki (University of East London, UK)\n\n\n\n\n\nThis talk will present a multi-modal approach to analyzing the complexity of environments and behaviors around infants, demonstrating automatic methods measuring both low-level salient features and high-level semantic features. We will introduce the use of spectral flux as a measure of acoustic change in the surrounding speech or in environmental sounds. In addition, we extract and quantify facial and hand movements (MMPose). Higher-level linguistic features, such as information rate and semantic surprisal, are derived from time-stamped automatic transcription (WhisperX) and processed using lossless compression techniques and GPT-2. Finally, facial novelty is measured as the Kullback-Leibler divergence between consecutive facial expressions. By integrating these multi-modal signals, we aim to provide a comprehensive framework for investigating the dynamic complexity of early environments.\n\n\n\n\n\n\n\n\n\nAnna Madden-Rusnak, Priyanka Khante, & Kaya de Barbaro (UT Austin, US)\n\n\n\n\n\nThis talk will describe progress and lessons learned drawing from our experience training classifiers to detect parent-infant behavior in context, including infant crying, parent holding, and household chaos. We will link specific examples from our experiences with guidelines for developmentalists interested in using and developing such models for their own work. These guidelines will highlight common pitfalls and challenges with using artificial intelligence for research and intervention, matched with best practices and practical recommendations for researchers working in this field."
  },
  {
    "objectID": "program.html#talk-session-3-using-machine-learning-to-track-learning-through-interactions-with-the-environment",
    "href": "program.html#talk-session-3-using-machine-learning-to-track-learning-through-interactions-with-the-environment",
    "title": "Detailed Program",
    "section": "Talk Session 3: Using machine learning to track learning through interactions with the environment",
    "text": "Talk Session 3: Using machine learning to track learning through interactions with the environment\nThe final talk session will discuss how to track learning through interactions with the environment by leveraging machine learning methods. Four talks will integrate research from developmental psychology, machine learning, and robotics on how infants and machines accelerate learning through embodied and multimodal interactions with their environment. Speakers will highlight research on how integrating vision, language, and action leads to greater learning efficiency, and how these findings can be applied to developing artificial intelligence that better reflects human-like learning.\n\n\n\n\n\n\nMelina Knabe (UT Austin, US)\n\n\n\n\n\nUnlike machines trained on extensive data, infants can extract regularities from limited - yet information-rich - input. How do they accomplish this feat? My talk will highlight work from our lab on how infants can track and integrate multimodal input statistics in their environment to accelerate learning. For instance, using machine learning techniques like word2vec we show that parents and infants jointly create semantic regularities during everyday toy play interactions. We will also discuss how integrating vision, language, and action leads to greater learning efficiency and how these insights can be applied to developing machines capable of learning more flexibly, efficiently, and autonomously, like a child.\n\n\n\n\n\n\n\n\n\nDaniel Messinger (University of Miami, US)\n\n\n\n\n\nMachine learning of data from child-worn sensors is shedding new light on early social and language development in group settings. Sensing movement and interpersonal orientation allows for data-driven approaches to inferring children when they are in social contact with other children and teachers. Movement/orientation data evidence homophily (children’s preference for similar others) at both the dyadic and larger group level. Machine learning of audio recordings suggests that child MLU can be measured reliably , indexing assessed language ability, and that the development of child MLU over a school year is associated with child-specific measures of .\n\n\n\n\n\n\n\n\n\nDavid Crandall (Indiana University, US)\n\n\n\n\n\nThis talk will highlight research efforts to develop machines that perceive and learn from the world in ways that parallel humans. Specifically, I will present ongoing work in my lab on designing highly performing video-and-language models and social robot conversation agents, demonstrating how insights from developmental science have contributed to infant- and toddler-inspired machine learning architectures and robots.\n\n\n\n\n\n\n\n\n\nJochen Triesch (Frankfurt Institute for Advanced Studies, Germany)\n\n\n\n\n\nInfants and toddlers learn about objects quite differently from today’s artificial intelligence systems. To better understand these processes, we develop computational models of how infants and toddlers acquire hierarchies of increasingly abstract object representations. These models learn from actual or simulated first-person visual input during extended interactions with objects. We show that strong object representations can result from just minutes of such first-person experience and highlight the benefits of toddlers’ gaze behavior for learning. Furthermore, we demonstrate the emergence of very abstract concepts such as kitchen object\" orgarden object” in these models."
  },
  {
    "objectID": "logistics.html",
    "href": "logistics.html",
    "title": "Logistics",
    "section": "",
    "text": "More details coming soon\n\n\n\nCheck back for more details about the location and other details closer to the date of the workshop."
  },
  {
    "objectID": "logistics.html#registration",
    "href": "logistics.html#registration",
    "title": "Logistics",
    "section": "Registration",
    "text": "Registration\nAll attendees (including presenters) must register for the workshop on the ICDL conference site. The registration for the workshop day is a separate fee than the main conference registration."
  },
  {
    "objectID": "logistics.html#poster-submissions",
    "href": "logistics.html#poster-submissions",
    "title": "Logistics",
    "section": "Poster Submissions",
    "text": "Poster Submissions\nPoster submissions are encouraged and will be accepted until September 1, 2025. Please indicate your interest here. The goal of the poster session is to showcase research related to using machine learning and wearable or environmental sensing technology to study early childhood behavior, environments, and learning. The poster session will be approximately 1.5 hours and provide an opportunity to discuss work with an interdisciplinary audience interested in developmental psychology, machine learning, and robotics. Attendees are welcome to present a poster that will also appear in the main conference as long as it fits the topic."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Workshop Overview",
    "section": "",
    "text": "The workshop “Machine Learning for Tracking Early Behavior and Environments” will take place on September 16, 2025 as part of the IEEE International Conference on Development and Learning in Prague, Czechia. Drawing 15 speakers with interdisciplinary expertise, the workshop will disseminate cutting-edge applications of machine learning methods in infancy research that span a wide range of topics—perception, sleep, movement, language, emotion, and social interaction.\nThe organizers, Sam Wass, John Franchak, and Melina Knabe, invite you to join for the day-long program by registering on the ICDL conference site. A morning poster session will provide an opportunity for attendees to discuss ongoing projects. The logistics page has more details about conference registration and poster submissions."
  },
  {
    "objectID": "index.html#schedule-at-a-glance",
    "href": "index.html#schedule-at-a-glance",
    "title": "Workshop Overview",
    "section": "Schedule at a glance",
    "text": "Schedule at a glance\nThree talk sessions will demonstrate how researchers use machine learning to assess what infants do (Session 1), what they experience (Session 2), and what they learn (Session 3). See the detailed program for speakers and talk abstracts for each session.\n\n\n\n\n\n\n\n\nTime\nEvent\nTopic\n\n\n\n\n9:00–10:30\nTalk Session 1\nUsing machine learning to track behavior\n\n\n10:30–11:00\nCoffee break\n\n\n\n11:00–12:30\nPoster Session\nAttendees present related work\n\n\n12:30–14:00\nLunch\n\n\n\n14:00–15:30\nTalk Session 2\nUsing machine learning to track environments—audio and video\n\n\n15:30–16:00\nCoffee Break\n\n\n\n16:00–17:30\nTalk Session 3\nUsing machine learning to track learning through interactions with the environment"
  }
]