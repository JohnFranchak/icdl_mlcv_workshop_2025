---
title: "Detailed Program"
---

## Talk Session 1: Using machine learning to track behavior

The first talk session will discuss using machine learning to track infants' behavior. Most research about infants' everyday behavior has relied on laborious human coding of video recordings. Recent advances in wearable technologies---movement sensors, ECG, headcams/eye trackers, and audio recorders---have enormous potential to measure continuous, full-day behavior. However, sensors do not directly measure behaviors of psychological interest, such as sleep/wake states, motor actions, joint attention, and infant-caregiver proximity. Speakers will demonstrate how they have leveraged machine learning algorithms to classify infant and caregiver behavior from wearable sensors.

::: {.callout-note icon=false appearance="simple" collapse="true" title="Kathryn Humphreys (Vanderbilt University, US)"}

My talk will discuss capturing the dynamics of caregiver–child proximity across early life using wearable sensing technology. Our lab developed the TotTag, a wearable device that measures physical proximity between caregivers and children twice per second, allowing unprecedented ecological assessment of how proximity patterns differ across families and contexts, change across development, and relate to  developmental outcomes. This approach enables continuous monitoring of caregiver–child interactions in natural settings. We are currently using this methodology in longitudinal studies tracking family proximity patterns from pregnancy through early childhood, capturing everyday experiences during developmental transitions that were previously difficult to measure outside laboratory settings.
:::

::: {.callout-note icon=false appearance="simple" collapse="true" title="John Franchak, Hanzhi Wang, & Hailey Rousey (University of California, Riverside, US)"}
Wearable movement sensors have the potential to describe real-time infant motor behavior in naturalistic settings, but raw sensor data need to be classified into relevant categories. We will present novel machine learning classifiers for infant posture (e.g., sitting, standing), locomotion (e.g., crawling, walking), and restraint (e.g., placed in seat and/or held) and describe results about each behavior from a dataset of 850 hours of in-home recording.
:::

::: {.callout-note icon=false appearance="simple" collapse="true" title="Nancy McElwain (University of Illinois, Urbana-Champaign, US)"}
Our interdisciplinary team has developed LittleBeats, an infant wearable platform that synchronously collects multimodal data (e.g., ECG, audio, motion) for extended periods of time (e.g., daylong recordings) in real-world contexts (e.g., the home). We use multimodal deep learning algorithms applied to LittleBeats data to assess a variety constructs, including infants’ regulation of emotions, sleep/wake states, and physical activity/sedentary behavior. We will present an example of these multimodal deep learning architectures for the detection of infant wake/sleep states, including REM and non-REM sleep stages. Advantages and challenges of multimodal machine learning approaches will be discussed.
:::

::: {.callout-note icon=false appearance="simple" collapse="true" title="Marwa Mahmoud (University of Glasgow, UK)"}
Automatic detection of behavioral cues from video offers promising tools for supporting the early diagnosis of neurodevelopmental conditions. In this talk, I will present our work on detecting specific infant movements---such as face-touch gestures---from video recordings, enabling insights into how babies engage with themselves and their environment. I will also discuss our results on the automatic analysis of videos during the Strange Situation Procedure to predict child attachment scores. In addition, I will present a multimodal machine learning framework that processes multiple behavioral signals to model behavior more holistically. Finally, I will highlight our recent efforts on interpretable, concept-based deep learning approaches, emphasising the importance of explainability in child behavior analysis.
:::

::: {.callout-note icon=false appearance="simple" collapse="true" title="Chen Yu (UT Austin, US)"}
I will first introduce HOME lab---a Home-like Observational Multimodal Environment (HOME) to study natural behavior. The HOME lab setup satisfies two different needs that usually cannot be achieved together: 1) The home-like environment elicits natural behaviors from parents and children; and 2) The environment is well-controlled. I will describe a set of everyday activities that we asked parents and children to engage in the HOME lab and the findings from those studies.
:::

## Talk Session 2: Using machine learning to track environments---Audio and video

The second talk session will discuss using machine learning to track early environments. Infants interact with their caregivers in context, and this environmental context crucially shapes developmental outcomes. For example, caregivers' speech input and sensitivity, as well as broader features of environments (e.g., noise, household chaos) can impact language and socio-emotional learning. How can researchers capture these environmental features using audio and video recordings? Speakers will discuss how they have applied machine learning algorithms to capture the early auditory and visual environment.

::: {.callout-note icon=false appearance="simple" collapse="true" title="Manjunath Mulimani (Tampere University, Finland)"}
This talk will present the ongoing research works on detection and classification of acoustic scene and events (DCASE) at the Signal Processing Research Centre. I will introduce acoustic scene classification, audio tagging and sound event detection. Further, I will highlight the issues and solutions associated with deep learning models for learning the DCASE tasks incrementally over existing knowledge. I will discuss the available audio pre-trained models, ways to extract the embeddings and solve target DCASE tasks. Finally, I will list all ongoing research works in our Audio Research Group and discuss collaboration opportunities.
:::

::: {.callout-note icon=false appearance="simple" collapse="true" title="Kaijia Tey (Paris, France)"}
Long-form recordings let us capture the richness of children's everyday language environments, offering insight into how they hear and use language in real life. Using tools like LENA and open-source alternatives (e.g., Voice Type Classifier (VTC), Lavechin et al. 2020), we measure key aspects of early language experience, including child vocalizations, adult speech, and conversational turns. I will share how we collect and analyze these recordings, what they reveal about language development across diverse communities, and the challenges we still face. This work highlights the potential and limitations of current methods in capturing children's linguistic experiences.
:::

::: {.callout-note icon=false appearance="simple" collapse="true" title="Einari Vaaras (Tampere University, Finland)"}
Understanding the emotional content of speech heard by infants is crucial for studying early emotional experiences and their impact on infants' developmental outcomes. This talk will present a state-of-the-art approach for automatic emotional analysis of child-directed speech in a neonatal intensive care unit environment. I will cover the development process of the approach, including data collection, the main challenges encountered, and the solutions implemented to address them. Additionally, I will briefly discuss the performance level of the proposed approach and the applications where the approach has been used.
:::

::: {.callout-note icon=false appearance="simple" collapse="true" title="Kaya de Barbaro, Anna Madden-Rusnak, & Priyanka Khante (UT Austin, US)"}
This talk will describe progress and lessons learned drawing from our experience training classifiers to detect parent-infant behavior in context, including infant crying, parent holding, and household chaos. I will focus on our recently completed work to develop a binary classifier for maternal sensitivity to infant distress using real world infant audio data (Macro F1 score = 0.76). Our findings demonstrate that integrating multi-scalar information (e.g. frames, events and states) improves performance over conventional frame-only models and that each scale provides unique, complementary information.
:::

::: {.callout-note icon=false appearance="simple" collapse="true" title="Pierre Labendzki (University of East London, UK)"}
This talk will present a multi-modal approach to analyzing the complexity of environments and behaviors around infants, demonstrating automatic methods measuring both low-level salient features and high-level semantic features. We will introduce the use of spectral flux as a measure of acoustic change in the surrounding speech or in environmental sounds. In addition, we extract and quantify facial and hand movements (MMPose). Higher-level linguistic features, such as information rate and semantic surprisal, are derived from time-stamped automatic transcription (WhisperX) and processed using lossless compression techniques and GPT-2. Finally, facial novelty is measured as the Kullback-Leibler divergence between consecutive facial expressions. By integrating these multi-modal signals, we aim to provide a comprehensive framework for investigating the dynamic complexity of early environments.
:::

::: {.callout-note icon=false appearance="simple" collapse="true" title="Manu Airaksinen (Helsinki, Finland)"}
Everyday human behavior occurs at broadly varying timescales involving an enormous array of possible patterns and states enabled by the biomechanics of the human body interacting with the varying environment. Current research into early life environments often examines highly specific features individually, by measuring (for example) how reduced exposure to adult talk associates with outcomes, and (separately) how unstable daily routines associate with outcomes, and so on. In reality, environmental features tend to cluster, such that (for example) children exposed to less adult talk are also likely to experience unstable daily routines, as well as early environments that differ in the amount of movement, positive/negative mood, noise, screen time, etc. One potential solution to this task is Causal Emergence (Hoel et al. 2013), which is the observation in network studies that in most natural real-world graphs (e.g., cell networks, molecular structures) the information theoretical measures of causality (e.g., effective information) exhibit a sweet spot within the dynamics of the graph when nodes are coarse-grained from the exact micro-scale representation to a meso- or macro-scale representation. 
:::

## Talk Session 3: Using machine learning to track learning through interactions with the environment

The final talk session will discuss how to track learning through interactions with the environment by leveraging machine learning methods. Four talks will integrate research from developmental psychology, machine learning, and robotics on how infants and machines accelerate learning through embodied and multimodal interactions with their environment. Speakers will highlight research on how integrating vision, language, and action leads to greater learning efficiency, and how these findings can be applied to developing artificial intelligence that better reflects human-like learning.

::: {.callout-note icon=false appearance="simple" collapse="true" title="Melina Knabe (UT Austin, US)"}

Unlike machines trained on extensive data, infants can extract regularities from limited - yet information-rich - input. How do they accomplish this feat? My talk will highlight work from our lab on how infants can track and integrate multimodal input statistics in their environment to accelerate learning. For instance, using machine learning techniques like word2vec we show that parents and infants jointly create semantic regularities during everyday toy play interactions. We will also discuss how integrating vision, language, and action leads to greater learning efficiency and how these insights can be applied to developing machines capable of learning more flexibly, efficiently, and autonomously, like a child.
:::

::: {.callout-note icon=false appearance="simple" collapse="true" title="Daniel Messinger (University of Miami, US)"}

Machine learning of data from child-worn sensors is shedding new light on early social and language development in group settings. Sensing movement and interpersonal orientation allows for data-driven approaches to inferring children when they are in social contact with other children and teachers. Movement/orientation data evidence homophily (children’s preference for similar others) at both the dyadic and larger group level. Machine learning of audio recordings suggests that child MLU can be measured reliably \textit{in vivo}, indexing assessed language ability, and that the development of child MLU over a school year is associated with child-specific measures of \textit{teacher}.

:::

::: {.callout-note icon=false appearance="simple" collapse="true" title="David Crandall (Indiana University, US)"}

This talk will highlight research efforts to develop machines that perceive and learn from the world in ways that parallel humans. Specifically, I will present ongoing work in my lab on designing highly performing video-and-language models and social robot conversation agents, demonstrating how insights from developmental science have contributed to infant- and toddler-inspired machine learning architectures and robots.

:::

::: {.callout-note icon=false appearance="simple" collapse="true" title="Jochen Triesch (Frankfurt Institute for Advanced Studies, Germany)"}
Infants and toddlers learn about objects quite differently from today’s artificial intelligence systems. To better understand these processes, we develop computational models of how infants and toddlers acquire hierarchies of increasingly abstract object representations. These models learn from actual or simulated first-person visual input during extended interactions with objects. We show that strong object representations can result from just minutes of such first-person experience and highlight the benefits of toddlers’ gaze behavior for learning. Furthermore, we demonstrate the emergence of very abstract concepts such as ``kitchen object" or ``garden object" in these models.
:::
